#!/usr/local/bin/python
# coding: latin-1
__author__ = "Laurent Perrinet INT - CNRS"
__licence__ = 'GPL licence'
DEBUG = True
DEBUG = False
"""
2020-09-25_IRPHE
"""

fig_width = 12

import os
# https://docs.python.org/3.8/library/pathlib.html
# import pathlib
from pathlib import Path

home = Path(os.environ['HOME'])

# figpath_talk = Path.cwd().joinpath('figures')
figpath_talk = 'figures'
Path.cwd().joinpath(figpath_talk).mkdir(parents=True, exist_ok=True)

# print(figpath_talk)
# figpath_slides = os.path.join(home, 'quantic/libraries/slides.py/figures/')
figpath_slides = 'https://laurentperrinet.github.io/slides.py/figures/'
#
import sys
print(sys.argv)
tag = sys.argv[0].split('.')[0]
if len(sys.argv) > 1:
    slides_filename = sys.argv[1]
else:
    slides_filename = None

from academic.cli import slugify
slugified = slugify(tag)

print('ğŸ˜ Welcome to the script generating the slides for ', tag)
try:
    YYYY = int(tag[:4])
    MM = int(tag[5:7])
    DD = int(tag[8:10])
except:
    from datetime import datetime
    DD = datetime.now().day
    MM = datetime.now().month
    YYYY = datetime.now().year

# see https://github.com/laurentperrinet/slides.py
from slides import Slides

height_ratio = .9
sections = ['*Natural* Vision', 'Deep Learning', 'Predictive processes', 'Perspectives']

meta = dict(
 embed=False,
 draft=DEBUG,  # show notes etc
 width=1600,
 height=1000,
 margin=0.15,
 reveal_path='https://laurentperrinet.github.io/2020-09-14_IWAI/reveal.js-master/',
 theme='simple',
 bgcolor="white",
 author='Perrinet, Laurent U',
 author_link=f'<a href="https://laurentperrinet.github.io/talk/{slugify(tag)}/">Laurent Perrinet</a>',
 title="Understanding natural vision <BR> using deep predictive coding",
 short_title="Understanding natural vision using deep predictive coding",
 conference="SÃ©minaire Ã  l'Institut de Recherche sur les PhÃ©nomÃ¨nes Hors Ã‰quilibre",
 conference_url='https://laurentperrinet.github.io/talk/2020-09-25-irphe',
 short_conference=' (IRPHÃ‰)',
 location='Marseille (France)',
 abstract="""Building models which efficiently process images is a great source of inspiration to better understand the processes which underly our visual perception. I will present some classical models stemming from the Machine Learning community and propose some extensions inspired by Nature. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computations at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision (CNNs in the terminology  of deep learning) take into account neurophysiological observations and provide as of today the most successful framework for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. We will here introduce a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective and we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational levels. These results from the SDPC model additionally demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.""",
 YYYY=YYYY, MM=MM, DD=DD,
 tag=tag,
 projects='aprovis-3-d',
 time_start='15:45:00',
 time_end='16:30:00',
 url=f'https://laurentperrinet.github.io/talk/{slugify(tag)}',
 sections=sections
)

###############################################################################
# ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„            QR CODE                      ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„
###############################################################################

# https://pythonhosted.org/PyQRCode/rendering.html
# pip3 install pyqrcode
# pip3 install pypng

# figname_qr = Path.cwd().joinpath(figpath_talk).joinpath('qr.png')
figname_qr = os.path.join(figpath_talk, 'qr.png')
if DEBUG or (not Path(figname_qr).is_file()):
    import pyqrcode as pq
    code = pq.create(meta['url'])
    code.png(figname_qr, scale=5)

print(meta['sections'])
s = Slides(meta)

###############################################################################
# ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„                INTRO                    ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„
###############################################################################
height_px = 120
# path_people = os.path.join(home, 'ownCNRS/2019-01_LACONEU/people')
path_people = home.joinpath('github/hugo_academic/content/authors')
# path_people = 'https://laurentperrinet.github.io/authors/'
# ls ~/github/hugo_academic/content/authors/**/avat*
People = ''
# People += s.content_imagelet(path_people.joinpath('chloe-pasturel/avatar.png'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('hugo-ladret/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('james-a-bednar/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('jens-kremkow/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('karl-friston/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('kiana-mansour-pour/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('laurent-u-perrinet/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('manuel-samuelides/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('maria-jose-escobar/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('mina-a-khoei/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('nicole-voges/avatar.gif'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('paula-s-leon/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('simon-j-thorpe/avatar.jpg'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('victor-boutin/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('wahiba-taouali/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('yves-fregnac/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('rick-a-adams/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('anna-montagnini/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('laurent-madelain/avatar.png'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('jean-bernard-damasse/avatar.jpg'), height_px, embed=True)
# People += s.content_imagelet(path_people.joinpath('stephane-viollet/avatar.jpg'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('alberto-vergani/avatar.jpg'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('angelo-franciosini/avatar.jpg'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('frederic-y-chavane/avatar.png'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('emmanuel-dauce/avatar.jpg'), height_px, embed=True)
People += s.content_imagelet(path_people.joinpath('etienne-rey/avatar.jpg'), height_px, embed=True)


s.meta['Acknowledgements'] = f"""
{People}
"""
# s.meta['Acknowledgements'] = f"""
# <small>
# <h5>Acknowledgements:</h5>
# <ul><li> Hakim El Hattab for <a href="https://revealjs.com/">reveal-js</a>
# <li>Some people...</li>
# <li>Lots more people...</li>
# </ul>
# <BR>
# {People}</a>
# <BR>
#     This work was supported by ....
# </small>
#
# """
s.open_section()
###############################################################################
intro = """
<h2 class="title">{title}</h2>
<h3>{author_link}</h3>
""".format(**meta)
# intro += s.content_imagelet('figures/ins-logo.png',
#                             s.meta['height']*.24,
#                             embed=False)
intro += s.content_imagelet('https://laurentperrinet.github.io/slides.py/figures/troislogos.png',
                            s.meta['height']*.32,
                            embed=False)  # bgcolor="black",
intro += """
<h4><a href="{conference_url}">{conference}{short_conference}</a> {DD}/{MM}/{YYYY} </h4>

{Acknowledgements}
""".format(**meta)
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################

s.add_slide(content=intro,
            notes="""
1

Bonjour je suis Laurent Perrinet et je vais vous parler aujourdâ€™hui de la vision naturelle. Je vais essayer de vous dÃ©montrer lâ€™importance que son Ã©tude peut avoir sur la construction dâ€™algorithmes de vision par ordinateur mais aussi pour nous mettre permettre dâ€™avoir une meilleure comprÃ©hension des principes neuronaux qui sous-tendent le fonctionnement de notre cerveau.

La vision est ce processus qui nous permet de faire sens Ã  partir du monde lumineux qui nous entoure. Je vous remercie de m'avoir invitÃ©, car son Ã©tude pourrait a priori par Ãªtre Ã©loignÃ© du domaine de recherche qui sont Ã©tudiÃ©s Ã  votre institut ... et je vais essayer de dÃ©montrer durant cet exposÃ© que la dÃ©raisonnable efficacitÃ© de la vision dans le monde naturel peuvent se comprendre comme des mÃ©canismes dâ€™intÃ©gration dynamique qui ne vous seront sÃ»rement pas Ã©tranges... On pourra donc espÃ©rer faire des ponts entre ces diffÃ©rents domaines dâ€™Ã©tudes.

""")
s.close_section()
###############################################################################
# ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ Vision ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„
###############################################################################
i_section = 0
###############################################################################
s.open_section()
s.add_slide_outline(i_section,
            notes="""2

            Durant cet exposÃ©, je vais prÃ©senter quelques principes fondamentaux, puis aussi prÃ©senter les techniques rÃ©centes dÃ©veloppÃ©es dans le cadre de lâ€™apprentissage profond. Enfin, je vais prÃ©senter nos travaux sur les modÃ¨les de processus productif pour la vision et proposer des perspectives de recherche.""")

for layer in ['brain', 'pathways', 'ventral']:
    s.add_slide(content=s.content_figures(
                [os.path.join(figpath_talk, f"architecture_vision_{layer}.svg")],
                height=s.meta['height']*height_ratio),
                notes="""

3

CommenÃ§ons par le dÃ©but et par lâ€™anatomie du systÃ¨me visuel. Cette figure montre une reprÃ©sentation schÃ©matique du cerveau chez lâ€™homme et en particulier le tronc cÃ©rÃ©bral, le cervelet et le neocortex.

4

De faÃ§on schÃ©matique on peut dÃ©crire l'anatomie de la vision comme suivant des chemins depuis lâ€™oeil, via le thalamus, pour converger sur l'aire corticale primaire de la vision (AKA V1). Depuis cette aire cÃ©rÃ©brale, lâ€™information est ensuite multiplexÃ©e entre une voie ventrale et une voie dorsale. La voie dorsale analyse prÃ©fÃ©rentiellement la temporalitÃ© de lâ€™information visuelle, comme par exemple le mouvement dans une image. Elle est donc essentielle par exemple pour guider les mouvement des yeux notamment quand ils servent Ã  compenser le mouvement d'un objet visuel.

5

La voie ventrale est principalement impliquÃ© dans la reconnaissance des formes comme par exemple la catÃ©gorisation des images visuelles. Ce processus peut lui-mÃªme Ãªtre dÃ©composÃ© en une sÃ©quence de diffÃ©rentes Ã©tapes de traitement de lâ€™information depuis la rÃ©tine et le thalamus, pour passer dans les aires visuelles primaires V1, V2, V3 jusquâ€™au aires corticales situÃ©e dans le lobe inferotemporal. Ainsi certaines neurones de ces aires peuvent Ãªtre sensible Ã  des caractÃ©ristiques visuelle trÃ¨s particuliÃ¨res comme par exemple des objets ou des visages, et ceci indÃ©pendamment de leur position dans lâ€™espace visuel. Cette organisation ressemble grossiÃ¨rement aux chaÃ®nes de traitement de lâ€™information que lâ€™on rencontre dans les algorithmes de vision par ordinateur, depuis l'entrÃ©e rÃ©tinienne jusquâ€™Ã  une sortie, par exemple motrice. Il est toutefois remarquable de noter que la vision nous paraÃ®t comme un mÃ©canisme tout Ã  fait naturel, et que le monde visuel me paraissent tout Ã  fait stable alors que ce systÃ¨me est soumis Ã  des contraintes majeures.
                """)

# author, year, journal, title='', url=None
bib = s.content_bib("LP", "2019", "Illusions et hallucinations visuelles : une porte sur la perception ", url="https://theconversation.com/illusions-et-hallucinations-visuelles-une-porte-sur-la-perception-117389")

for url in ['https://laurentperrinet.github.io/2020-09-14_IWAI/figures/CNS-general-II-A.png',
            'https://images.theconversation.com/files/277070/original/file-20190529-192383-zy01r.jpg']:
    fig = s.content_figures([url], bgcolor="black", height=s.meta['height']*height_ratio)

    s.add_slide(content=fig + bib,
                notes="""
6

Il faut tout dâ€™abord noter que dÃ¨s la rÃ©tine, lâ€™information visuelle (ici chez lâ€™homme) est fortement focalisÃ©e. En effet la majoritÃ© de lâ€™information est concentrÃ©e sur la macula, qui est la rÃ©gion autour de lâ€™axe optique. Par lâ€™anatomie de lâ€™Å“il que lâ€™on voit sur cette figure  et qui correspond Ã  la majoritÃ© des "pixels", câ€™est-Ã -dire des photos rÃ©cepteurs placÃ© sur le fond de lâ€™Å“il et qui constitue le senseur sur la rÃ©tine. SchÃ©matiquement on peut illustrer cette transformation sur une image classique en A, jusquâ€™Ã  sa transformation dans une carte de type log-polaire de telle sorte quâ€™on peut comprimer lâ€™image en une reprÃ©sentation en C, et que lâ€™on peut aussi reprÃ©senter dans lâ€™espace visuel sous la forme de la figure D. Cette transformation de lâ€™espace visuel induit une forte distorsion de lâ€™image qui est difficilement imaginable, et des illusions dâ€™optique permet de le rÃ©vÃ©ler. Un exemple sont les illusions qui mettent en Ã©vidence la tache aveugle câ€™est-Ã -dire le point oÃ¹ la sortie de la rÃ©tine converge sur  le nerf optique et qui n'est pas photosensible. Certaines illusions utilisent des configurations pour lesquelles cette distorsion induit une certaine instabilitÃ©â€¦

7

Un exemple cette illusion dit-il dites dites des serpents qui tournoient. Cette illusion crÃ©e par le professeur Akiyoshi Kitaoka induit des hallucinations de mouvements dans une image qui nâ€™en contient pas. Bien que lâ€™image soit totalement statique, lâ€™interfÃ©rence des patterns concentriques interfÃ¨rent avec la gÃ©omÃ©trie de la rÃ©tine. Cela induit des illusions de mouvement et de faÃ§on globale la stabilitÃ© de la figure gÃ©omÃ©trique...
                """)

# author, year, journal, title='', url=None
bib = s.content_bib("LP", "2020", "Temps et cerveau : comment notre perception nous fait voyager dans le temps", url="https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567")

for url in ['https://laurentperrinet.github.io/2019-04-18_JNLF/figures/scheme_thorpe.jpg',
            'https://laurentperrinet.github.io/2019-04-18_JNLF/figures/figure-tsonga.png',
            'https://upload.wikimedia.org/wikipedia/commons/6/60/Flash_lag.gif',
            ]:
    fig = s.content_figures([url], bgcolor="black", height=s.meta['height']*height_ratio)

    s.add_slide(content=fig + bib, notes="""

8

Une autre contrainte auxquelles notre cerveau est confrontÃ© et le fait que lâ€™information visuelle mette un certain temps pour passer dâ€™un cotÃ© Ã  lâ€™autre le long des voies visuelles. Je reproduis ici une figure Ã©tablie par Simon Thorpe qui montre chez le macaque les diffÃ©rentes latence de lâ€™information visuelle quand elle voyage depuis la rÃ©tine jusquâ€™Ã  une rÃ©ponse motrice. Ainsi si une image est prÃ©sentÃ©e au temps zÃ©ro elle sortira de la rÃ©tine aprÃ¨s environ 30 millisecondes, puis sera traitÃ© dans lâ€™Ã¨re visuelle primaire aprÃ¨s 50 millisecondes pour Ãªtre traitÃ© le long de la voie ventrale en 100 millisecondes. Cette information pourra ensuite Ãªtre traitÃ©e pour atteindre les aires motrices pour Ã©tablir une commande motrice et enfin une rÃ©ponse en environ 200 ms. Câ€™est l'ordre de grandeur du temps quâ€™il faut pour rÃ©pondre Ã  une image et qui est observÃ© aussi chez les humains. Je vais maintenant vous illustrer les consÃ©quences de ses dÃ©lais neuronaux.

9

En effet si lâ€™on considÃ¨re une situation relativement naturel relevant du domaine de la vision câ€™est-Ã -dire ici Jau Wilfried son gars son gars Ã  vous le donne en train en train dâ€™essayer de rattraper passer une chatte est une chatte est un chat pour une vitesse pour une balle se dÃ©plaÃ§ant Ã  une vitesse dâ€™environ 20 m/s.
En rapportant les dÃ©lais neuronaux par rapport Ã  la balle Ã  lâ€™instant prÃ©sent telle quâ€™elle est au centre de lâ€™image, On nâ€™en dÃ©duit que lâ€™image qui est vu par le cerveau dans lâ€™air visuel primaire dans les airs de reconnaissance le long de la voie dorsale par exemple et dÃ©caler dans le temps de sâ€™ennuyer souvent et donc dans lâ€™espace dâ€™environ 1 m.
De la mÃªme faÃ§on, la position de la balle la position de la balle par rapport Ã  une action possible prise Ã  ce moment-lÃ  câ€™est-Ã -dire Ã  lâ€™instant prÃ©sent doit Ãªtre effectuÃ© pour une position de Laval telle quâ€™elle sera 80 000 secondes aprÃ¨s le temps prÃ©sent câ€™est-Ã -dire en avant dâ€™une distance Ã©quivalente Ã  Ã  peu prÃ¨s 80 cm.
Devant lâ€™urgence de pouvoir rÃ©pondre de faÃ§on efficace Ã  une telle scÃ¨ne, on comprend que le systÃ¨me visuel a Ã©tÃ© optimisÃ© durant les millions dâ€™annÃ©es de la sÃ©lection de lâ€™Ã©ternel. Pour permettre de rÃ©agir efficacement Ã  ce genre de situation.
Ce genre dâ€™optimisation de la vision des processus de la vision peut Ãªtre mis en Ã©vidence dans une illusion extrÃªme moment simple dite du flash retardÃ©.


10

Dans cette illusion, la consigne est de fixer le point vert au centre de lâ€™Ã©cran. Si maintenant je vous demande de me donner la position du point rouge au moment oÃ¹ le point vert apparaÃ®t, vous me direz sans doute il apparaÃ®t lÃ©gÃ¨rement Ã  droite de celui ci.
Pourtant, Ã  lâ€™instant oÃ¹ le point vert apparaÃ®t, le point rouge est exactement Ã  la verticale du point vert. Ceci est dÃ» au fait que le point vert apparaÃ®t de faÃ§on non prÃ©dictive alors que le point rouge, qui suit qui suit une trajectoire rectiligne et continue, est lui reprÃ©sentÃ© par le systÃ¨me visuel en avant de sa trajectoire. Cela implique aussi le systÃ¨me visuel ne regarde pas de faÃ§on passive lâ€™information rÃ©tinienne mais plutÃ´t quâ€™il perÃ§oit une information telle qu'elle est transformÃ©e par les processus visuels. On voit pas ce quâ€™on voit mais on voit ce quâ€™on croit.

Par consÃ©quent, on a pu mettre en Ã©vidence par ces simples expÃ©riences la dÃ©raisonnable efficacitÃ© du systÃ¨me visuel.

    """)

s.close_section()
###############################################################################
# ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„       Deep Learning  - 15''              ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„
###############################################################################
i_section = 1
###############################################################################
s.open_section()
s.add_slide_outline(i_section,
notes="""

11

Examinons maintenant lâ€™Ã©tat de l'art de nos connaissances en vision par ordinateur et en particulier des progrÃ¨s rÃ©cents effectuÃ©s en apprentissage profond.

""")

# author, year, journal, title='', url=None
bib = s.content_bib("Matthew Ricci and Thomas Serre", "2020", "Hierarchical Models of the Visual System", url="https://link.springer.com/referenceworkentry/10.1007%2F978-1-4614-7320-6_345-2")

for layer in ['ventral', 'cnn']:
    s.add_slide(content=s.content_figures(
                [os.path.join(figpath_talk, f"architecture_vision_{layer}.svg")],
                height=s.meta['height']*height_ratio) + bib,
                notes="""

12

On a vu tout Ã  lâ€™heure que lâ€™anatomie du systÃ¨me visuel dessine,  en ce qui concerne la reconnaissance dâ€™image, une chaÃ®ne de traitement de lâ€™information le long de la voie ventrale.

13

Une telle architecture est largement utilisÃ©e dans des modÃ¨les hiÃ©rarchique du systÃ¨me visuel comme dÃ©crit Ã  dans cette revue qui va transformer une image dâ€™entrÃ©e Ã  travers diffÃ©rentes couches prototypiques de traitement, jusquâ€™Ã  une reprÃ©sentation progressivement plus abstraite jusquâ€™Ã  obtenir une reconnaissance sous forme sÃ©mantique.

                """)


fig = s.content_figures(
            [os.path.join(figpath_talk, f"Serre_Fig{i}_DL.png") for i in ['3', '4']],
            height=s.meta['height']*height_ratio, fragment=True)

s.add_slide(content=fig + bib, notes="""

14
Ce type dâ€™architecture consiste Ã  dÃ©composer les niveaux dâ€™analyse en des processus  traitant progressivement des caractÃ©ristiques visuelles du particulier au gÃ©nÃ©ral.
Dans la premiÃ¨re couche (V1), nous avons un traitement qui va extraire les contours de lâ€™image pour ensuite analyser les courbure de ces diffÃ©rents contours, pour ensuite introduire des reprÃ©sentations plus complexes et abstraites et enfin obtenir des unitÃ©s de classification ("un renard"). MathÃ©matiquement, on peut formaliser ce type de modÃ¨le sous la forme dâ€™un rÃ©seau de neurones convolutionnel et qui constitue la base de la grande majoritÃ© des algorithmes dâ€™apprentissage profond qui sont utilisÃ©s de nos jours.
Lâ€™opÃ©ration de base correspond Ã  un filtrage linÃ©aire qui utilise un noyau conditionnel et qui va reprÃ©senter la corrÃ©lation de ce noyau avec des sous partie de lâ€™image. Cette opÃ©ration est rÃ©pÃ©tÃ©e dans une mÃªme couche sur diffÃ©rents canaux et va permettre par exemple de reprÃ©senter sur une mÃªme carte les diffÃ©rentes orientations des contours sur cette carte. Avant de propager ce traitement dans une nouvelle couche une Ã©tape importante et dâ€™opÃ©rer une transformation non linÃ©aire non linÃ©aire qui correspondent Ã  une rectification et Ã  une intÃ©gration par exemple en Ã©liminant un neurone sur deux.
On voit ainsi que au fur et Ã  mesure que lâ€™on monte dans la hiÃ©rarchie, les champs rÃ©cepteurs deviennent progressivement de plus en plus grands comme on peut voir ici en comparant les champs rÃ©cepteur RF1 et RF2.

""")


# author, year, journal, title='', url=None
bib = s.content_bib("Sadek Alaoui", "2017", "Convolutional Neural Network", url="http://sqlml.azurewebsites.net/2017/09/12/convolutional-neural-network/")
fig = s.content_figures([os.path.join(figpath_talk, 'null-38.png')], bgcolor="black", height=s.meta['height']*height_ratio)
s.add_slide(content=fig + bib, notes="""
15

GrÃ¢ce notamment aux progrÃ¨s dans les algorithmes d'apprentissage (la retro propagation du gradient), dans le matÃ©riel (les GPUs) mais aussi dans l'organisation de la communautÃ© pour obtenir des benchmarks efficaces, ces algorithmes ont rapidement progressÃ© sur une tache complexe de reconnaissance (IamgeNet) jusqu'Ã  atteindre la performance humaine (5%) en 2015. Ce progrÃ¨s vient au prix d'une explosion de la complexitÃ© de ces rÃ©seaux, mesurÃ©e ici avec le nombre de couches des CNNs.
    """)
# author, year, journal, title='', url=None
bib = s.content_bib("Dario Amodei & Danny Hernandez", "2018", "OpenAI", url="https://openai.com/blog/ai-and-compute/")
fig = s.content_figures([os.path.join(figpath_talk, 'ai-and-compute-all.png')], bgcolor="black", height=s.meta['height']*height_ratio)
s.add_slide(content=fig + bib, notes="""

16

En regardant sur une Ã©chelle temporelle plus large, on voit que c'est en tendance suivez la loi de Moor depuis les annÃ©es 1960 et a clairement explosÃ© depuis l'introduction de l'apprentissage profondâ€¦ C'est vraiment une nouvelle Ã¨re qui vient de s'ouvrir !

    """)


# author, year, journal, title='', url=None
bib = s.content_bib("Thomas Serre", "2019", "Deep Learning: The Good, the Bad, and the Ugly", url="https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-091718-014951")

for url in  [f'https://www.annualreviews.org/na101/home/literatum/publisher/ar/journals/content/vision/2019/vision.2019.5.issue-1/annurev-vision-091718-014951/20190909/images/large/vs50399.f{i}.jpeg' for i in ['3', '4']]:
    fig = s.content_figures([url], bgcolor="black", height=s.meta['height']*height_ratio)

    s.add_slide(content=fig + bib, notes="""

17

En dehors des progrÃ¨s effectuÃ©s dans la catÃ©gorisation d'images (ce qui est trÃ¨s important pour les entreprises qui financent ces recherche -Facebook, Amazon, ou Google- on a vu l'Ã©mergence de nouvelles techniques qui montre toute la crÃ©ativitÃ© de la communautÃ© en apprentissage profond. En suivant cet article de revue de Thomas Serre, je citerai par exemple la capacitÃ© de pouvoir faire du transfert de style c'est-Ã -dire de pouvoir transformer une image dans le style d'un peintre donnÃ©. Ou alors la crÃ©ation d'images complÃ¨tement nouvelles comme par exemple montrer dans le panneau d,  ou alors dans cette peinture dans le panneau B qui a Ã©tÃ© crÃ©Ã© par un algorithme gÃ©nÃ©ratif et qui a mÃªme atteint un prix mirobolant dans une vente aux enchÃ¨res....

18

Mais tout n'est pas rose dans le monde de l'apprentissage profondâ€¦
En effet, ces modÃ¨les qui ont gagnÃ© le concours de reconnaissance d'image peuvent Ãªtre trÃ¨s sensibles Ã  des petites perturbations qui va faire qu'une chaussette va Ãªtre reconnu comme un Ã©lÃ©phant, qu'un panneau de signalisation ne pourra pas Ãªtre reconnu. si il contient des bouts de sketch de scotch, , qu'une personne va changer d'une entitÃ© si elle porte des lunettes. Pire, des algorithmes qui atteignent des niveaux de performance sur-humains seront incapables de gÃ©nÃ©raliser leurs performances si on change le type de perturbation qui est appliquÃ©e aux images.
Il existe d'autres faiblesses, comme le manque d'interprÃ©tabilitÃ©, mais...


    Figure 3â€‚ Art and image synthesis with deep neural networks. (a) Style transfer with convolutional neural networks (CNNs). The content of an image (top, first panel) is combined with the style of three distinct paintings (top, second, third, and fourth panels) to synthesize a new artistic image (bottom row) using a neural network (Gatys et al. 2017). Image credit: Matthias Bethge (adapted with permission). (b) The Portrait of Edmond Belamy produced by a generative adversarial network (GAN) and sold by Christie's for $432,500 in October 2018. Reproduced with permission from the copyright holder. Sotheby's Contemporary Art Day Auction held in March 2019 also featured a machine installation that used neural networks to generate an infinite stream of portraits. Mainstream artists including Refik Anadol, Trevor Paglen, and Jason Salavon have started to incorporate neural networks as a part of their artistic process. (c) Latest improvements in style transfer by leveraging attentional mechanism to produce transfers that respect the semantic content of the original image (Park & Lee 2019). Image credit: Dae Y. Park and Kwang H. Lee (adapted with permission). (d) Synthetic images generated by a large generative adversarial network named BigGAN (Brock et al. 2019). Adapted with permission from the authors.

    """)

# TODO : lee sedol
fig = s.content_figures(['https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/power.png'], height=s.meta['height']*height_ratio)
s.add_slide(content=fig + bib, notes="""

19

... un des pires aspects est la comparaison avec l'intelligence naturelle. Ã€ ce titre le match d'Alpha Go, s'il constitue un milestone dans l'apprentissage par renforcement, constitue aussi une illustration de la grande diffÃ©rence d'Ã©nergie dÃ©ployÃ©e dans cet algorithme par rapport au 5W nÃ©cessaires pour alimenter un cerveau humain...

""")

s.close_section()
###############################################################################
# ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„       Predictive processes - 15''        ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„
###############################################################################
i_section = 2
###############################################################################
s.open_section()
s.add_slide_outline(i_section,
notes="""

20

Dans le suite, nous allons essayer d'explorer des alternatives, comme par exemple le codage prÃ©dictif.

""")


# author, year, journal, title='', url=None
SDPC_bib = s.content_bib('Boutin, Franciosini, Chavane, Ruffier, LP', '2020', 'submitted',
            url="https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20")

bib = s.content_bib("Matthew Ricci and Thomas Serre", "2020", "Hierarchical Models of the Visual System", url="https://link.springer.com/referenceworkentry/10.1007%2F978-1-4614-7320-6_345-2")

# for layer in ['ventral', 'cnn']:
for layer in ['cnn']:
    s.add_slide(content=s.content_figures(
                [os.path.join(figpath_talk, f"architecture_vision_{layer}.svg")],
                height=s.meta['height']*height_ratio) + bib,
                notes="""

21

En effet, nous avons vu que la vision artificielle considÃ¨re des processus "en avant", mais si l'on zoome sur une partie de l'aire V1 par exemple...

                """)


url_talk = 'https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/'
# author, year, journal, title='', url=None
bib = s.content_bib("Bosking et al.", "1997", "Journal of Neuroscience", url="https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/#/0/4")
fig = s.content_figures([f'{url_talk}Bosking97Fig4.jpg'], bgcolor="black", height=s.meta['height']*height_ratio)
s.add_slide(content=fig + bib, notes="""
22

on dÃ©couvre alors qu'il existe un riche rÃ©seau de connections latÃ©rales (dÃ©notÃ©es ici comme des points noirs au dessus de la reprÃ©sentation des orientations en couleurs). Des analyses ont montrÃ© que ces connections sont largement majoritaires en nombre, de l'ordre de 95% du nombre de boutons synaptiques.
""")

bib = s.content_bib("Bastos et al.", "2012", "Canonical Microcircuits for Predictive Coding", url="http://dx.doi.org/10.1016/j.neuron.2012.10.038")

for figname in ["Canonical_Microcircuit.svg", "Bastos12Fig5.png"]:
    s.add_slide(content=s.content_figures(
            [
                os.path.join(figpath_talk, figname),
                # os.path.join(figpath_talk, "Bastos12Fig5.png"),
            ],
            height=s.meta['height']*height_ratio, fragment=True) + bib,
            notes="""

23

Ces connections participent plus gÃ©nÃ©ralement Ã  un circuit prototypique, le micro-circuit cortical, qui intÃ¨gre Ã  la fois le traitement en avant, que les connections latÃ©rales mais aussi les connections en retour qui proviennent d'aires "supÃ©rieures"...

24

.. et pourraient Ãªtre compris comme l'implÃ©mentation d'un circuit prÃ©dictif qui minimiserait la surprise d'un modÃ¨le interne. Le champion de cette modÃ©lisation est Karl Friston de l'UCL.

            """)


#
# for suffix in ['_a', '_b', '_c', '']:
#     s.add_slide(content=s.content_figures(
#         [os.path.join(url_talk, 'boutin-franciosini-ruffier-perrinet-19_figure1' + suffix + '.png')], bgcolor="black",
#     title=None, embed=False, height=s.meta['height']*.85)+SDPC_bib,
#            notes="""
#
# figure 1 of SDPC
#
# cf. research/NN-2018
#
# """)

for suffix in ['_FF', '_RNN', '']:
    s.add_slide(content=s.content_figures(
        [os.path.join(figpath_talk, 'BoutinFranciosiniChavaneRuffierPerrinet20' + suffix + '.svg')], bgcolor="black",
    title=None, embed=False, height=s.meta['height']*height_ratio)+SDPC_bib,
           notes="""


25

BasÃ©s sur ce genre de principe, nous avons Ã©tendu un modÃ¨le convolutionnel classique comportant des couches convolutionnelles...

26

... un processus rÃ©current intra cortical ...

27

Mais aussi des connections rÃ©-entrantes (ici dÃ©notÃ©es en bleu). Dans une telle architecture, l'algorithme de retro propagation du gradient n'est plus possible. Nous avons introduit un algorithme d'apprentissage plus rÃ©alistes biologiquement car il est 1/local Ã  l'aire corticale et qui s'applique Ã  cette architecture, 2/ totalement auto-supervisÃ©, c'est-Ã -dire qu'il ne dÃ©pend pas d'une tache de catÃ©gorisation.

""")

# s.add_slide(content="""
#     <video controls loop width=85%/>
#       <source type="video/mp4" src="{}">
#     </video>
#     """.format(f'{url_talk}training_video_ATT.mp4'),
#     notes="""
#
#     Let's now apply that to natural images of faces
#
# """)
s.add_slide(content="""
    <video controls loop width=85%/>
      <source type="video/mp4" src="{}">
    </video>
    """.format(f'{url_talk}training_video_ATT.mp4')+SDPC_bib,
    notes="""

28

EntrainÃ© sur une base d'image de visage, cet algorithme permet de voir Ã©merger des noyaux de convolution dans la premiÃ¨re et la deuxiÃ¨me couche du rÃ©seau. Il est remarquable de remarquer, qu'Ã  la diffÃ©rence d'un CNN classique, les noyaux de convolutions sont interprÃ©tables et correspondent Ã  des bords (couche 1) mais aussi Ã  de "proto-concepts" comme ces champs rÃ©cepteurs sensibles Ã  des bouts de visage: oeil, nez, bouche...

    """)
# 20190206-Training of the SDPC model on AT&T database-0CFrmgEcGpw.f135.mp4

# for suffix in ['4a', '4b']:
#     s.add_slide(content=s.content_figures(
#         [os.path.join(url_talk, 'boutin-franciosini-ruffier-perrinet-19_figure' + suffix + '.png')], bgcolor="black",
#     title=None, embed=False, height=s.meta['height']*height_ratio),
#            notes="""
#
# Multi-layered unsupervised Learning
#
# Figure 4: Input, features and reconstructed input after the training on the AT&T database.(a) Images pre-processed with Local Contrast Normalization [36]. (b) 32 randomly selectedfirst-layer RFs fromD(1).  (c) First-layer reconstruction, generated by back-projectingÎ³1into  the  input  space.   (d)  64  second-layer  RFs  (randomly  selected)  of  size  28Ã—28  px,drawn from the effective dictionaryD(2).  (e) Second-layer reconstruction, generated byback-projectingÎ³2into the input space, showing 3 best examples (left) and the 3 worst.
#
# """)
s.add_slide(content=s.content_figures(
    [os.path.join(figpath_talk, 'PCOMPBIOL-D-19-01811_R2_compressed_Fig' + suffix + '.png') for suffix in ['2', '9']], title=None, fragment=True, height=s.meta['height']*height_ratio)+SDPC_bib,
       notes="""
29

Ces rÃ©sultats s'Ã©tendent aussi bien aux images naturelles qu'Ã  des images de visage et permettent une reprÃ©sentation progressivement
Pendant sa thÃ¨se, Victor Boutin a rÃ©ussi Ã  prouver que ce rÃ©seau permettait notamment de Ã©bruiter des images naturelles de faÃ§on robuste, comme cet oiseau et sa reconstruction Ã  partir de l'information dans les couches 1 & 2 (colonnes) et respectivement avec ou sans connections rÃ©-entrantes. Ces rÃ©sultats ont Ã©tÃ© quantifiÃ©s dans les panneaux B & C.

""")
for suffix in ['S4']:
    s.add_slide(content=s.content_figures(
        [os.path.join(figpath_talk, 'PCOMPBIOL-D-19-01811_R2_compressed_Fig' + suffix + '.png')], bgcolor="black",
    title=None, fragment=True, height=s.meta['height']*height_ratio)+SDPC_bib,
           notes="""

30

De faÃ§on surprenante, on a aussi vu Ã©merger dans le rÃ©seau des pattern de connection qui module lÃ© rÃ©ponse de neurones voisins, comme par exemple ceux qui sont alignÃ©s ou co-circulaires Ã  un bord orientÃ©, de faÃ§on similaire Ã  ce que nous avons observÃ© dans V1.

""")
#
# s.add_slide(content=s.content_figures(
#     [os.path.join('https://github.com/VictorBoutin/SPC_2L/raw/master/Savings/Fig/Fig7.png')],
#     title=None, embed=False, height=s.meta['height']*height_ratio)+SDPC_bib,
#        notes="""
#
# """)

# s.add_slide(content=s.content_figures(
#     [os.path.join(url_talk, 'SDPC_' + suffix + '.png') for suffix in ['3', '4']],
#     bgcolor="black", fragment=True,
#     title=None, embed=False, height=s.meta['height']*.75),
#        notes="""
#
# allows for a better classification as here for MNIST digits
# """)

s.close_section()
###############################################################################
# ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„             Perspectives      10''       ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„
###############################################################################
i_section = 3
###############################################################################
s.open_section()
s.add_slide_outline(i_section, notes="""

31

MalgrÃ© la rÃ©ussite de ce modÃ¨le, il reste toutefois de nombreuses perspectives Ã  aborder dans ce domaine de recherche. Notamment, je voudrais revenir au...

""")
#
# fname = os.path.join(home, 'quantic/science/ActiveVision/2020-09-14_IWAI/2020-09-10_video-abstract.mp4')
# s.add_slide(content=f"""<video controls autoplay loop width=60%/><source type="video/mp4" src="{fname}"></video>""",
#             notes="""You can also embed videos.""")

bib = s.content_bib("Emmanuel DaucÃ©, Pierre AlbigÃ¨s & LP", "2020", "Journal of Vision", url="https://laurentperrinet.github.io/publication/dauce-20/")

url_talk = 'https://laurentperrinet.github.io/2020-09-14_IWAI/figures/'

for layer in ['brain', 'pathways']:
    s.add_slide(content=s.content_figures(
                [os.path.join(figpath_talk, f"architecture_vision_{layer}.svg")],
                height=s.meta['height']*height_ratio),
                notes="""

32

... cerveau...

33

... et Ã  l'existence de deux voies de traitement...

34

... ainsi qu'Ã  l'existence d'un forte compression de l'information autour de l'axe optique.

                """)


for url in ['https://laurentperrinet.github.io/2020-09-14_IWAI/figures/CNS-general-II-A.png',
            # 'https://images.theconversation.com/files/277070/original/file-20190529-192383-zy01r.jpg'
            ]:
    fig = s.content_figures([url], bgcolor="black", height=s.meta['height']*height_ratio)

    s.add_slide(content=fig, # + bib,
                notes="""

34

... ainsi qu'Ã  l'existence d'un forte compression de l'information autour de l'axe optique.


35

Imaginons alors une simple expÃ©rience psychophysique dans laquelle je vous demande de fixer la croix,

36

 puis de reconnaitre un chiffre en pÃ©riphÃ©rie

38-40

ou au centre...

41-46

si la prÃ©sentation est rapide, cette tache peut-Ãªtre difficile, notamment car si le chiffre est en pÃ©riphÃ©rie, sa rÃ©solution dÃ©croit et donc il ne peut pas Ãªtre dÃ©chiffrÃ© (Ã  proprement parler) sans faire une saccade vers la position de ce chiffre. on parle alors d'infÃ©rence contre-factuelle, car on doit deviner l'action Ã  mener en devinant la performance obtenue en effectuant cette action. Ce genre de problÃ¨me est extrÃªmement courant en vision  d'oÃ¹ l'importance d'introduire ce type d'action dans la vision par ordinateur.
Nous avons alors fait l'hypothÃ¨se que les deux voies (ventrale et dorsale) sont dÃ©diÃ©es Ã  deux taches diffÃ©rente

                """)

for i in [0, 4, 8, 9]:
    s.add_slide(image_fname=f'{url_talk}film_FIX.png')
    s.add_slide(image_fname=f'{url_talk}film_display{i}.png')
    s.add_slide(image_fname=f'{url_talk}film_display{i}_SAC.png')
    # s.add_slide(image_fname=f'{url_talk}film_ANS.png')



####################### SLIDE B 2 ##################################
subtitle = [': Computational Graph']
for i, fname in enumerate(['fig_methods']): # CNS-what-where-diagram
    s.add_slide(content=s.content_figures(
    [f'{url_talk}{fname}.svg'],
    height=s.meta['height']*height_ratio) + bib,
    notes="""

47

d'un cotÃ©, la voie dorsale va identifier Ã  partir de la vision pÃ©riphÃ©rique une estimation de la probabilitÃ© de trouver la position d'un objet, indÃ©pendamment de son identitÃ©. cela permet d'effectuer une saccade qui une fois rÃ©alisÃ©e permettra de la tester grÃ¢ce Ã  la voie ventrale.


    """)

#
# ####################### SLIDE B 3 ##################################
# subtitle = [': What']
# for i, fname in enumerate(['CNS-what-diagram']):
#     s.add_slide(content=s.content_figures(
#     [f'{url_talk}{fname}.svg'],
#     # title=title + subtitle[i],
#     height=s.meta['height']*height_ratio) + bib,
#     notes="""
#
#
# WHAT :
#
# On the one side, a ventral pathway predicts the target identity from the current foveal data.
#
# The what pathway is a classic convolutional clasifier.
#
# It shows some translation invariance.
#
# It can quantify its uncertainty.
#
# It monitors the where pathway.
#
#     TODO: mettre le rÃ©sultat de l'accuracy map pour faire la transition?
#
#
#     """)
#
#
# ####################### SLIDE B 4 ##################################
# subtitle = [': Where']
# for i, fname in enumerate(['fig_where']):
#     s.add_slide(content=s.content_figures(
#     [f'{url_talk}{fname}.svg'],
#     # title=title + subtitle[i],
#     height=s.meta['height']*height_ratio) + bib,
#     notes="""
#
# WHERE :
#
# On the other side, a dorsal pathway utilizes all the peripheral visual data.
#
# Here we make the assumption that the same logpolar compression pattern is conserved from the retina up to the primary motor layers.
# **Each possible future saccade has an expected accuracy, that can be trained from the what pathway output**. To accelerate the training, we use a shortcut that is training the network on a translated accuracy map (with logpolar encoding). The ouput is thus a **logpolar accuracy map**, that tells for each possible visuo-motor displacement the value of the future accuracy.Thus, the saccadic motor ouput (colliculus) shows a similar log-polar compression than the visual input. The saccades are more precise at short than at long distance (and severals accades may be necessary to precisely reach distant targets).
#     """)
# """The prediction takes the form of an **acuracy map**, that predicts the increase of accuracy for different possible saccades.
#
# This accuracy map is organized radially, with a higher spatial definition at the center than at the periphery. """

title = 'Active vision' # meta['sections'][i_section]

#for kind in ['correct', 'error']:
s.add_slide(content=s.content_figures(
[f'{url_talk}CNS-saccade-' + str(idx) + '.png' for idx in [8, 20]],
        # title=title + ': success',
        height=s.meta['height']*.5, transpose=True) + bib,
notes="""

** The effect of a saccade is to bring relevant visual data at the fovea**

(from left to right)

1. The full visual field

2. After a log polar encoding, the peripheral target is less visible

3. this is the true acuracy map

4. this is the predicted acc map

5. this is the visual data collected at the fovea after the saccade.



48

de faÃ§on surprenante, nous avons pu montrer qu'une telle prÃ©diction contre-factuelle de la position est possible, mÃªme avec des images pÃ©riphÃ©riques trÃ¨s dÃ©gradÃ©es et permettent aussi d'obtenir une bonne catÃ©gorisation

""")

s.add_slide(content=s.content_figures(
[f'{url_talk}CNS-saccade-' + str(idx) + '.png' for idx in [46, 47, 32] ],
        # title=title + ': failure',
        height=s.meta['height']*.6, transpose=True) + bib,
notes="""

49

avec aussi des cas d'erreur, soit dans la prediction (chiffre dans le bruit?), soit dans la prÃ©cision ou mÃªme le classifier.


""")

s.add_slide(content=s.content_figures(
[f'{url_talk}results-IG.png'],
     # title="Effect of eccentricity",
     height=s.meta['height']*height_ratio) + bib,
notes = """

50

de faÃ§on cruciale, nous montrons qu'avec une simple saccade et un coÃ»t computation minime, on peut couvrir une large surface de l'image. comparÃ©s Ã  des algorithmes classiques de localisation / classification comme YOLO, nous obtenons un cout computationnel sous-linÃ©aire grÃ¢ce Ã  la compression log-polaire.

           """)



s.add_slide(content=s.content_figures(
            [os.path.join(figpath_talk, "2020-05-20_AAPG2020_AgileNeuroBot.png")],
            height=s.meta['height']*height_ratio),
            notes="""


51

nous avons eu la bonne surprise la semaine derniÃ¨re d'obtenir un financement ANR pour inclure ce genre d'approches bio-inspirÃ©es sur un drone en collaboration avec Stephane Viollet Ã  l'ISM et Ryad Benosman Ã  l'IdV.


            """)

s.close_section()
###############################################################################
# Conclusion - 1''
###############################################################################
###############################################################################
s.open_section()
s.add_slide_summary(title='Conclusion',
    list_of_points =['vision is efficient',
                     'deep learning is great...',
                     '... but predictive coding is better',
                     '... let''s make it active ! '],
                     fragment_type=None,
                     # fragment_type='highlight-green', # BROKEN
    notes="""

52

Pour conclure, la vision est effectivement dÃ©raisonnablement efficace, l'apprentissage profond est un bon moyen de s'approcher de cette efficacitÃ© - et de nouvelles approches de codage prÃ©dictif ou de vision active permettrons je l'espÃ¨re de bientÃ´t dÃ©passer ces derniers.
""")
s.add_slide(content=intro,
            notes="""

53

Merci pour votre attention! et Ã  mes collaborateurs:

""")

s.add_slide(content=s.content_figures([figname_qr], #title=f"""<a href="{meta['url']}">{meta['url']}</a>""",
 cell_bgcolor=meta['bgcolor'], height=s.meta['height']*height_ratio) + '<BR><a href="{url}"> {url} </a>'.format(url=meta['url']),
            notes="All the material is available online - please flash this code this leads to a page with links to further references and code ")

s.close_section()

###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################


if slides_filename is None:
    with open("README.md", "w") as text_file:
        text_file.write("""\
# {title}

* What:: talk @ [{conference}{short_conference}]({conference_url})
* Who:: {author}
* Where: {location}, see {url}
* When: {DD:02d}/{MM:02d}/{YYYY}, time: {time_start}-{time_end}

* What:
  * Slides @ https://laurentperrinet.github.io/{tag}
  * Code for slides @ https://github.com/laurentperrinet/{tag}/
  * Abstract: {abstract}

""".format(**meta))

    with open("/tmp/talk.bib", "w") as text_file:
        text_file.write("""\
@inproceedings{{{tag},
    Author = "{author}",
    Booktitle = "{conference}",
    Title = "{title}",
    Abstract = "{abstract}",
    Url = "{url}",
    Year = "{YYYY}",
    Date = "{YYYY}-{MM:02d}-{DD:02d}",
    location = "{location}",
    projects = "{projects}",
    time_start = "{YYYY}-{MM:02d}-{DD:02d}T{time_start}",
    time_start = "{YYYY}-{MM:02d}-{DD:02d}T{time_end}",
    url = "{url}",
    url_slides = "https://laurentperrinet.github.io/{tag}",
    url_code = "https://github.com/laurentperrinet/{tag}/",
}}

""".format(**meta))

else:
    s.compile(filename=slides_filename)

# Check-list:
# -----------
#
# * (before) bring miniDVI adaptors, AC plug, remote, pointer
# * (avoid distractions) turn off airport, screen-saver, mobile, sound, ... other running applications...
# * (VP) open monitor preferences / calibrate / title page
# * (timer) start up timer
# * (look) @ audience
#
# Preparing Effective Presentations
# ---------------------------------
#
# Clear Purpose - An effective image should have a main point and not be just a collection of available data. If the central theme of the image isn't identified readily, improve the paper by revising or deleting the image.
#
# Readily Understood - The main point should catch the attention of the audience immediately. When trying to figure out the image, audience members aren't fully paying attention to the speaker - try to minimize this.
#
# Simple Format - With a simple, uncluttered format, the image is easy to design and directs audience attention to the main point.
#
# Free of Nonessential Information - If information doesn't directly support the main point of the image, reserve this content for questions.
#
# Digestible - Excess information can confuse the audience. With an average of seven images in a 10-minute paper, roughly one minute is available per image. Restrict information to what is extemporaneously explainable to the uninitiated in the allowed length of time - reading prepared text quickly is a poor substitute for editing.
#
# Unified - An image is most effective when information is organized around a single central theme and tells a unified story.
#
# Graphic Format - In graphs, qualitative relationships are emphasized at the expense of precise numerical values, while in tables, the reverse is true. If a qualitative statement, such as "Flow rate increased markedly immediately after stimulation," is the main point of the image, the purpose is better served with a graphic format. A good place for detailed, tabular data is in an image or two held in reserve in case of questions.
#
# Designed for the Current Oral Paper - Avoid complex data tables irrelevant to the current paper. The audience cares about evidence and conclusions directly related to the subject of the paper - not how much work was done.
#
# Experimental - There is no time in a 10-minute paper to teach standard technology. Unless the paper directly examines this technology, only mention what is necessary to develop the theme.
#
# Visual Contrast - Contrasts in brightness and tone between illustrations and backgrounds improves legibility. The best color combinations include white letters on medium blue, or black on yellow. Never use black letters on a dark background. Many people are red/green color blind - avoid using red and green next to each other.
#
# Integrated with Verbal Text - Images should support the verbal text and not merely display numbers. Conversely, verbal text should lay a proper foundation for each image. As each image is shown, give the audience a brief opportunity to become oriented before proceeding. If you will refer to the same image several times during your presentation, duplicate images.
#
# Clear Train of Thought - Ideas developed in the paper and supported by the images should flow smoothly in a logical sequence, without wandering to irrelevant asides or bogging down in detail. Everything presented verbally or visually should have a clear role supporting the paper's central thesis.
#
# Rights to Use Material - Before using any text, image, or other material, make sure that you have the rights to use it. Complex laws and social rules govern how much of someone's work you can reproduce in a presentation. Ignorance is no defense. Check that you are not infringing on copyright or other laws or on the customs of academic discourse when using material.
#
# http://pne.people.si.umich.edu/PDF/howtotalk.pdf
#
